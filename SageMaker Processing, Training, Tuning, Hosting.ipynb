{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod a+x data_downloader.sh\n",
    "!bash data_downloader.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load labels dataset\n",
    "base_dir = \"data/\"\n",
    "df = pd.read_csv(os.path.join(base_dir, \"labels.csv\"))\n",
    "print(df.shape)\n",
    "\n",
    "# Add path of each image as a new column\n",
    "image_dict = {}\n",
    "for x in glob(os.path.join(base_dir, \"images\", \"*.jpg\")):\n",
    "    image_dict[os.path.splitext(os.path.basename(x))[0]] = x\n",
    "print(len(image_dict))\n",
    "df['path'] = df['image_id'].map(image_dict.get)\n",
    "\n",
    "\n",
    "# Pre-process labels: full name + categorize\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'Melanoma',\n",
    "    'bkl': 'Benign keratosis-like lesions ',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}\n",
    "\n",
    "df['cell_type'] = df['dx'].map(lesion_type_dict.get)\n",
    "df['cell_type_idx'] = pd.Categorical(df['cell_type']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import image\n",
    "\n",
    "i = 11\n",
    "print(df['cell_type'][i])\n",
    "img = image.load_img(df['path'][i], target_size=(256, 256))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Data Processing \n",
    "Turn images and the label vectors into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile processing.py\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import argparse\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from tensorflow.python.keras.preprocessing import image\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "def load_images(path):\n",
    "    \"\"\"Load images from local and resize them\"\"\"\n",
    "    \n",
    "    # ---- Read the condition's image and append\n",
    "    img = image.load_img(path, target_size=(32, 32))\n",
    "    \n",
    "    # ---- Covert to array, and \"preprocess it\" so that keras model can read it\n",
    "    img_array = image.img_to_array(img)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "\n",
    "def _load_train_dataset(base_dir):\n",
    "    \"\"\"Load images from local, create dataframe and split in train and test\"\"\"\n",
    "    \n",
    "    # Read labels csv (labels + image metadata/paths)\n",
    "    df = pd.read_csv(os.path.join(base_dir, \"labels.csv\"))\n",
    "    print(\"Loaded Labels DF with Shape {}\".format(df.shape))\n",
    "    \n",
    "    # Add path of each image as a new column\n",
    "    image_dict = {}\n",
    "    for x in glob(os.path.join(base_dir, \"images\", \"*.jpg\")):\n",
    "        image_dict[os.path.splitext(os.path.basename(x))[0]] = x\n",
    "\n",
    "    df['path'] = df['image_id'].map(image_dict.get)\n",
    "    \n",
    "    # Pre-process labels: full name + categorize\n",
    "    lesion_type_dict = {\n",
    "        'nv': 'Melanocytic nevi',\n",
    "        'mel': 'Melanoma',\n",
    "        'bkl': 'Benign keratosis-like lesions ',\n",
    "        'bcc': 'Basal cell carcinoma',\n",
    "        'akiec': 'Actinic keratoses',\n",
    "        'vasc': 'Vascular lesions',\n",
    "        'df': 'Dermatofibroma'\n",
    "    }\n",
    "    df['cell_type'] = df['dx'].map(lesion_type_dict.get)\n",
    "    df['cell_type_idx'] = pd.Categorical(df['cell_type']).codes\n",
    "    \n",
    "    \n",
    "    # Load images into the df\n",
    "    print(\"Resizing + converting images into numpy arrays ...\")\n",
    "    df['image'] = df['path'].map(lambda x: load_images(x))\n",
    "    \n",
    "    # Train/Test split\n",
    "    features = df['image']\n",
    "    target = df['cell_type_idx']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create class weights\n",
    "    cls_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "    # Convert images and the label vectors into numpy arrays    \n",
    "    y_train = to_categorical(y_train, num_classes=7)\n",
    "    y_test = to_categorical(y_test, num_classes=7)\n",
    "    x_train = np.asarray(x_train.tolist()).reshape((x_train.shape[0],*(32,32,3)))\n",
    "    x_test = np.asarray(x_test.tolist()).reshape((x_test.shape[0],*(32,32,3)))\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, cls_weight\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "    parser.add_argument('--local_path', type=str, default=\"/opt/ml/processing\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    if args.local_path == \"/opt/ml/processing\":\n",
    "        base_dir = os.path.join(args.local_path, 'input/')\n",
    "    else:\n",
    "        base_dir = args.local_path\n",
    "    \n",
    "    x_train, y_train, x_test, y_test, cls_weight = _load_train_dataset(base_dir)\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(x_test.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    np.save(os.path.join(args.local_path, \"train/cls_weight.npy\"), cls_weight)\n",
    "    np.save(os.path.join(args.local_path, \"train/x_train.npy\"), x_train)\n",
    "    np.save(os.path.join(args.local_path, \"train/y_train.npy\"), y_train)\n",
    "    np.save(os.path.join(args.local_path, \"test/x_test.npy\"), x_test)\n",
    "    np.save(os.path.join(args.local_path, \"test/y_test.npy\"), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data/train\n",
    "!mkdir data/test\n",
    "\n",
    "%run -i processing.py \\\n",
    "    --local_path ./data/\n",
    "\n",
    "!ls data/train/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Model Training\n",
    "Test-drive a simple tf.keras training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0' \n",
    "\n",
    "def instantiate_classifier():\n",
    "    \"\"\"Generate & compile a simple CNN classifier using tf.keras sequential API\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.20))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(7, activation='softmax'))\n",
    "    \n",
    "    METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "    ]\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=METRICS\n",
    "                 )\n",
    "\n",
    "    return model\n",
    "\n",
    "def _parse_args():\n",
    "    \"\"\"Parse user-defined command line arguments\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir', type=str)\n",
    "    parser.add_argument(\"--epochs\", type=int)\n",
    "    parser.add_argument(\"--batch_size\", type=int)\n",
    "    parser.add_argument(\"--model_version\", type=str)\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Parse command line args\n",
    "    args, unknown = _parse_args()\n",
    "\n",
    "    # Load processed data (numpy arrays)\n",
    "    cls_weight = np.load(args.train + \"/cls_weight.npy\")\n",
    "    x_train = np.load(args.train + \"/x_train.npy\")\n",
    "    y_train = np.load(args.train + \"/y_train.npy\")\n",
    "    x_test = np.load(args.test + \"/x_test.npy\")\n",
    "    y_test = np.load(args.test + \"/y_test.npy\")\n",
    "\n",
    "    # Instantiate classifier\n",
    "    classifier = instantiate_classifier()\n",
    "    \n",
    "    # Fit classifier\n",
    "    classifier.fit(x_train, y_train,\n",
    "                   batch_size=args.batch_size,\n",
    "                   epochs=args.epochs,\n",
    "                   class_weight=cls_weight,\n",
    "                   verbose=1\n",
    "                  )\n",
    "    \n",
    "    # Print AUC on test-set\n",
    "    print(\"AUC: \" + str(classifier.evaluate(x_test, y_test)[-1]))\n",
    "    \n",
    "    classifier.save(os.path.join(args.model_dir, args.model_version), 'my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models\n",
    "\n",
    "%run -i train.py \\\n",
    "    --model_dir ./models/ \\\n",
    "    --train ./data/train/ \\\n",
    "    --test ./data/test/ \\\n",
    "    --model_version 2 \\\n",
    "    --batch_size 32 \\\n",
    "    --epochs 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Data Processing, Training, Tuning, Hosting & Monitoring\n",
    "\n",
    "Before we start using SageMaker, let's copy our dataset to S3. S3 is the main data store for SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 client\n",
    "import boto3\n",
    "region='us-east-1'\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "# Create S3 Buckets for this project\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "BUCKET = \"skin-cancer-classifier-{account_id}\".format(account_id=account_id)\n",
    "s3_client.create_bucket(Bucket=BUCKET)\n",
    "\n",
    "print(\"Project Bucket: {bucket}\".format(bucket=BUCKET))\n",
    "\n",
    "!aws s3 cp data/images/ {\"s3://{}/raw-data/images/\".format(BUCKET)} --recursive\n",
    "\n",
    "!aws s3 cp data/labels.csv {\"s3://{}/raw-data/labels.csv\".format(BUCKET)}\n",
    "\n",
    "!aws s3 ls {\"s3://\" + BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to tag all sagemaker jobs/artifacts with a date/time string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "WORKFLOW_DATE_TIME = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Processing jobs\n",
    "\n",
    "With Amazon SageMaker Processing jobs, you can leverage a simplified, managed experience to run data pre- or post-processing and model evaluation workloads on the Amazon SageMaker platform.\n",
    "\n",
    "A processing job downloads input from Amazon Simple Storage Service (Amazon S3), then uploads outputs to Amazon S3 during or after the processing job.\n",
    "\n",
    "<img src=\"README-IMAGES/Processing-1.jpg\">\n",
    "\n",
    "Note that:\n",
    "\n",
    "    1. Common use case is to run a scikit-learn script that cleans, pre-processes, performs feature-engineering, and splits the input data into train and test sets.\n",
    "\n",
    "    2. However, you can also run a post-processing jobs on the test data to evaluate a trained model's performance\n",
    "\n",
    "    3. You can take advantage of SageMaker's pre-built scikit-learn, spark and popular deep learning containers or use your own custom container to run processing jobs with your own Python libraries and dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Processing Job Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "train_path = \"s3://{}/{}/data/train\".format(BUCKET, WORKFLOW_DATE_TIME)\n",
    "test_path = \"s3://{}/{}/data/test\".format(BUCKET, WORKFLOW_DATE_TIME)\n",
    "\n",
    "inputs = [ProcessingInput(source=\"s3://{}/raw-data/\".format(BUCKET),\n",
    "                          destination='/opt/ml/processing/input',\n",
    "                          s3_data_distribution_type='ShardedByS3Key'\n",
    "                         )\n",
    "         ]\n",
    "\n",
    "outputs = [ProcessingOutput(output_name='train',\n",
    "                            destination=train_path,\n",
    "                            source='/opt/ml/processing/train'\n",
    "                           ),\n",
    "           ProcessingOutput(output_name='test',\n",
    "                            destination=test_path,\n",
    "                            source='/opt/ml/processing/test'\n",
    "                           )\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ScriptProcessor` class in the SageMaker SDK lets you run a command inside this container, which you can use to run your own script.\n",
    "\n",
    "For a full list of available container URIs, see [Available Deep Learning Container Images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md) for more information on using Docker containers, see Use Your Own Algorithms or Models with Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "script_processor = ScriptProcessor(\n",
    "    image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:1.15.2-cpu-py37-ubuntu18.04\",\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    role=role,\n",
    "    command=['python3']\n",
    ")\n",
    "                                    \n",
    "script_processor.run(job_name=\"skin-cancer-processing-{}\".format(WORKFLOW_DATE_TIME),\n",
    "                     code='processing.py',\n",
    "                     inputs=inputs,\n",
    "                     outputs=outputs,\n",
    "                     arguments=['--train-test-split-ratio', '0.2']\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring Your Own Container\n",
    "This is the Dockerfile to create the processing container. Install `pandas`, `scikit-learn`, `Pillow` and `TensorFlow` into it. You can install your own dependencies.\n",
    "\n",
    "```shell\n",
    "!mkdir docker\n",
    "```\n",
    "\n",
    "```python\n",
    "%%writefile docker/Dockerfile\n",
    "\n",
    "FROM python:3.7-slim-buster\n",
    "\n",
    "RUN pip3 install pandas==0.25.3 scikit-learn==0.21.3 Pillow==5.4.1 tensorflow==1.15.2 Keras==2.2.4 Keras-Applications==1.0.8 Keras-Preprocessing==1.1.0\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]\n",
    "```\n",
    "\n",
    "This block of code builds the container uri in AWS ECR\n",
    "```python\n",
    "import boto3\n",
    "region = \"us-east-1\"\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "uri_suffix = 'amazonaws.com'\n",
    "ecr_repository = 'sagemaker-processing-containers'\n",
    "tag = ':latest'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "```\n",
    "\n",
    "This block of code builds the container using the `docker` command, creates an Amazon Elastic Container Registry (Amazon ECR) repository, and pushes the image to Amazon ECR.\n",
    "```shell\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker hosted training \n",
    "There are four modes to SageMaker:\n",
    "* **Built-in**: Choose one of our 17 built-in algorithms and simply point SageMaker to your data on S3.\n",
    "* **Script Mode**: Author your own model using SKlearn, Tensorflow, PyTorch or MXNet.\n",
    "* **BYO Container**: Very similar to script-mode but with one additional parameter that tells SageMaker to use one of your own custom docker containers.\n",
    "* **Marketplace**: Purchase an algorithms from 100s of third-party sellers and simply point SageMaker to your data on S3.\n",
    "![modes](README-IMAGES/sagemaker-training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Mode\n",
    "We provide SageMaker with a training script that simply loads the processed data, which has been copied to the container from S3, and fits a simple CNN multi-class classifier.\n",
    "\n",
    "At the end of the training job we have added a step to export the trained model to the path stored in the environment variable **SM_MODEL_DIR**, which always points to **/opt/ml/model**. This is critical because SageMaker uploads all the model artifacts in this folder to S3 at end of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will submit job by creating a TensorFlow Estimator. The sagemaker.tensorflow.TensorFlow estimator handles locating the script mode container, uploading your script to a S3 location and creating a SageMaker training job. Let's call out a couple important parameters here:\n",
    "\n",
    "* py_version is set to 'py3' to indicate that we are using Python 3 and script mode. \n",
    "* entry_point is set to the name of our python training script\n",
    "* hyperparameters is a dictionary containing values to model hyperparameters and other arguments needed to run our script. Example: model_version is not an hyperparameter but a way to version our model. \n",
    "\n",
    "To start a training job, we call estimator.fit(training_data_uri).\n",
    "\n",
    "An S3 location is used here as the input. fit creates a default channel named 'train', which points to this S3 location. In the training script we can then access the training data from the location stored in SM_CHANNEL_TRAINING. fit accepts a couple other types of input as well. See the API doc [here](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase.fit) for details.\n",
    "\n",
    "When training starts, the TensorFlow container executes train.py, passing hyperparameters and model_dir from the estimator as script arguments. \n",
    "\n",
    "When training is complete, the training job will upload the saved model for TensorFlow serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       hyperparameters={\n",
    "                           'epochs': 10,\n",
    "                           'batch_size': 64,\n",
    "                           'model_version': '2'\n",
    "                       },\n",
    "                       train_instance_count=1,\n",
    "                       train_instance_type='ml.m5.xlarge',\n",
    "                       output_path = 's3://{}/{}'.format(BUCKET, WORKFLOW_DATE_TIME + '/model-artifacts'),\n",
    "                       code_location = 's3://{}/{}'.format(BUCKET, WORKFLOW_DATE_TIME + '/source-code'),\n",
    "                       role=role,\n",
    "                       framework_version=\"2.1.0\",\n",
    "                       py_version='py3',\n",
    "                       model_dir=\"/opt/ml/model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(job_name = \"skin-cancer-{}\".format(WORKFLOW_DATE_TIME),\n",
    "              inputs = {'train': train_path, 'test': test_path},\n",
    "              wait = True,\n",
    "              logs=False\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Model Tuning\n",
    "\n",
    "So far we have simply run one Local Mode training job and one Hosted Training job without any real attempt to tune hyperparameters to produce a better model, other than increasing the number of epochs. Selecting the right hyperparameter values to train your model can be difficult, and typically is very time consuming if done manually. The right combination of hyperparameters is dependent on your data and algorithm; some algorithms have many different hyperparameters that can be tweaked; some are very sensitive to the hyperparameter values selected; and most have a non-linear relationship between model fit and hyperparameter values. SageMaker Automatic Model Tuning helps automate the hyperparameter tuning process: it runs multiple training jobs with different hyperparameter combinations to find the set with the best model performance.\n",
    "\n",
    "We begin by specifying the hyperparameters we wish to tune, and the range of values over which to tune each one. We also must specify an objective metric to be optimized: in this use case, we'd like to minimize the validation loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'learning_rate': ContinuousParameter(0.001, 0.2, scaling_type=\"Logarithmic\"),\n",
    "    'epochs': IntegerParameter(10, 50),\n",
    "    'batch_size': IntegerParameter(64, 256),\n",
    "}\n",
    "\n",
    "metric_definitions = [{'Name': 'AUC',\n",
    "                       'Regex': 'AUC: ([0-9\\\\.]+)'\n",
    "                      }]\n",
    "objective_metric_name = 'AUC'\n",
    "objective_type = 'Maximize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify a HyperparameterTuner object that takes the above definitions as parameters. Each tuning job must be given a budget: a maximum number of training jobs. A tuning job will complete after that many training jobs have been executed.\n",
    "\n",
    "We also can specify how much parallelism to employ, in this case five jobs, meaning that the tuning job will complete after three series of five jobs in parallel have completed. For the default Bayesian Optimization tuning strategy used here, the tuning search is informed by the results of previous groups of training jobs, so we don't run all of the jobs in parallel, but rather divide the jobs into groups of parallel jobs. There is a trade-off: using more parallel jobs will finish tuning sooner, but likely will sacrifice tuning search accuracy.\n",
    "\n",
    "Now we can launch a hyperparameter tuning job by calling the fit method of the HyperparameterTuner object. The tuning job may take around 10 minutes to finish. While you're waiting, the status of the tuning job, including metadata and results for invidual training jobs within the tuning job, can be checked in the SageMaker console in the Hyperparameter tuning jobs panel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=5,\n",
    "                            objective_type=objective_type\n",
    "                           )\n",
    "\n",
    "tuning_job_name = \"skin-cancer-{}\".format(WORKFLOW_DATE_TIME)\n",
    "tuner.fit(job_name=tuning_job_name,\n",
    "          inputs={'train': train_path, 'test': test_path}\n",
    "         )\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tuning job is finished, we can use the HyperparameterTuningJobAnalytics object from the SageMaker Python SDK to list the top 5 tuning jobs with the best performance. Although the results vary from tuning job to tuning job, the best validation loss from the tuning job (under the FinalObjectiveValue column) likely will be substantially lower than the validation loss from the hosted training job above, where we did not perform any tuning other than manually increasing the number of epochs once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "tuner_metrics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "tuner_metrics.dataframe().sort_values(['FinalObjectiveValue'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total training time and training jobs status can be checked with the following lines of code. Because automatic early stopping is by default off, all the training jobs should be completed normally. For an example of a more in-depth analysis of a tuning job, see the SageMaker official sample [HPO_Analyze_TuningJob_Results.ipynb](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = tuner_metrics.dataframe()['TrainingElapsedTimeSeconds'].sum() / 3600\n",
    "print(\"The total training time is {:.2f} hours\".format(total_time))\n",
    "tuner_metrics.dataframe()['TrainingJobStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker hosted endpoint\n",
    "\n",
    "Assuming the best model from the tuning job is better than the model produced by the individual Hosted Training job above, we could now easily deploy that model to production. A convenient option is to use a SageMaker hosted endpoint, which serves real time predictions from the trained model (Batch Transform jobs also are available for asynchronous, offline predictions on large datasets). The endpoint will retrieve the TensorFlow SavedModel created during training and deploy it within a SageMaker TensorFlow Serving container. This all can be accomplished with one line of code.\n",
    "\n",
    "More specifically, by calling the deploy method of the HyperparameterTuner object we instantiated above, we can directly deploy the best model from the tuning job to a SageMaker hosted endpoint. It will take several minutes longer to deploy the model to the hosted endpoint compared to the Local Mode endpoint, which is more useful for fast prototyping of inference code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_predictor = tuner.deploy(initial_instance_count=1,\n",
    "                                instance_type=\"ml.m5.xlarge\",\n",
    "                                endpoint_type=\"tensorflow-serving\",\n",
    "                                endpoint_name = \"skin-cancer-classifier\"\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native support for data-capture\n",
    "```python\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "                        enable_capture = True,\n",
    "                        sampling_percentage=50,\n",
    "                        destination_s3_uri='s3://tf-dermatology/endpoint-traffic/',\n",
    "                        kms_key_id=None,\n",
    "                        capture_options=[\"REQUEST\", \"RESPONSE\"],\n",
    "                        csv_content_types=[\"text/csv\"],\n",
    "                        json_content_types=[\"application/json\"]\n",
    ")```\n",
    "\n",
    "### add the new configuration and wait for it to be applied\n",
    "```python\n",
    "from sagemaker import RealTimePredictor\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=\"tf-dermatology\")\n",
    "predictor.update_data_capture_config(data_capture_config=data_capture_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the endpoint\n",
    "\n",
    "The formats of the input and the output data correspond directly to the request and response formats of the Predict method in the [TensorFlow Serving REST API](https://www.tensorflow.org/serving/api_rest). SageMaker's TensforFlow Serving endpoints can also accept additional input formats that are not part of the TensorFlow REST API, including the simplified JSON format, line-delimited JSON objects (\"jsons\" or \"jsonlines\"), and CSV data.\n",
    "\n",
    "In this example we are using a numpy array as input, which will be serialized into the simplified JSON format. In addtion, TensorFlow serving can also process multiple items at once as you can see in the following code. You can find the complete documentation on how to make predictions against a TensorFlow serving SageMaker endpoint [here](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst#making-predictions-against-a-sagemaker-endpoint).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "test_index = [1000,2000,3000,4000,5000]\n",
    "for i in test_index:\n",
    "    image_path = df['path'][i]\n",
    "    img = image.load_img(image_path, target_size=(32, 32))\n",
    "    img_array = image.img_to_array(img)\n",
    "    imgs.append(img)\n",
    "print(len(imgs))\n",
    "\n",
    "test = np.stack(imgs)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs = {\n",
    "  'instances': test\n",
    "}\n",
    "result = tuning_predictor.predict(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the enpoint from anywhere! Using Lambda and API Gateway\n",
    "[APIGateway](https://console.aws.amazon.com/apigateway/home?region=us-east-1#/apis/obrgi23zgl/resources/u208pq/methods/POST)\n",
    "\n",
    "![modes](README-IMAGES/lambda-apigateway.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "Let's delete the endpoint we just created to prevent incurring any extra costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(tuning_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
